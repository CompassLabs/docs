---
sidebar_position: 3
title: Basic Concepts
---

<!-- import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem'; -->
import ImgBacktest from '../static/img/schema_backtesting.png';
import ImgOptimization from '../static/img/schema_optimization.png';
import ImgGeneral from '../static/img/env4.png';
import ImgLoop from '../static/img/env3.png';


`dojo` runs a simulation loop, combining principles from [reinforcement learning](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology) ğŸ¤– and [agent-based modelling](https://en.wikipedia.org/wiki/Agent-based_model) ğŸ§‘â€ğŸ¤â€ğŸ§‘.
This flexible framework enables you to simulate and explore a diverse range of scenarios that reflect DeFi dynamics.   

The schematic below shows the simplified representation of the agent-environment loop. 

<img src={ImgGeneral} alt="aasd" width="40%" />


To make things easier, letâ€™s consider an example of a trader trading on Uniswap V3 ğŸ¦„ğŸ¦„ğŸ¦„:  

1. ğŸ£ **Environment creation**: as a first step in the simulation, an environment is created. In this case UniswapV3.  
2. â®ï¸ **Reset environment**: the environment is reset to get the first observation from the environment. Observations provide information on the
state of the environment. For example, the trader might query the current price in a pool.
3. ğŸ‹ï¸ **Run policies**: these obervations are turned into actions via your policies. Think of the trader submitting a trade based on the current price in a pool.  
4. â¯ï¸ **Process actions**: the environment executes these actions on the forked-blockchain. This can be imagined as Uniswap processing the trader's trade. 
The observations change and the agents compute their rewards for taking their actions.

The agent rewards are useful in two cases: [static policy performance](./Basic_Concepts#-static-policy-performance) and [optimizing a policy](./Basic_Concepts#-optimizing-a-policy).

:::info `dojo` is still in early stage
Become an early adopter and help us shape the product in the way you need ğŸ™‚
:::

## ğŸ¥‡ Static policy performance
If your policy is static (non-parameter based), the agentâ€™s reward function is simply a measure of performance at every step in the simulation loop.  

<img src={ImgGeneral} alt="aasd" width="50%" />

<!-- <img src=require('@site/static/img/simulation.png').default alt="aasd" width="50px" /> -->

## ğŸ“ˆ Optimizing a policy
If you want to optimize model parameters to improve your policy performance, dojoâ€™s feedback loop feeds the reward function back into the policy to iteratively enhance the decision-making process and generate better actions taken by your agent. 

<img src={ImgLoop} alt="aasd" width="50%" />

___
## ğŸ’¬ Terminology

### ğŸŒ Environments

:::note Environments
Environments represent DeFi protocols (e.g. Uniswap V3 ğŸ¦„ or Balancer V2 âš–ï¸).
:::

Environments represent the setting in which an agent interacts and possibly learns. At every simulation step (every block on the blockchain),
the environment **emits an observation ğŸ” and rewards ğŸ¥‡** generated by the agents in the simulation. The observation provides information on the state of the environment
at the current simulation block.

The agents' policies process the observation and rewards to take a sequence of actions. The actions ğŸ•º are executed inside the environment. 
In practice this means dojo makes the appropriate **smart contract call to our local fork of the DeFi protocol**,
and emits new observations, i.e. the updated state of the chain, back to the simulation.


See [here](./environments/Overview) for more details.

### ğŸ§‘â€ğŸ¤â€ğŸ§‘ Agents

:::note Agents
Agents represent the state of the actors interacting with the environment (e.g. traders)
:::

The environment has reference to all agents, each of which keeps track of their own cryptocurrency quantities. 
They also implement their own reward function for **metric tracking**, and, optionally, **policy training**.

:::info

Agents are **NOT** responsible for making decisions on how to act in the environment.

:::

See [here](./agents) for more details.

### ğŸ§  Policies

:::note Policies
Policies determine the behavior of the agents.
:::

**This is where you can get creative by implementing your own policy for interacting with the environment!** For example, a basic policy could be the [moving average trading strategy](https://www.investopedia.com/ask/answers/122314/how-do-i-use-moving-average-ma-create-forex-trading-strategy.asp).  

At every simulation step (i.e., every block), the policy receives an observation ğŸ” from the environment, with which it generates a sequence of actions ğŸ•º to pass back. Optionally, it also receives a reward ğŸ¥‡ which can be used to train or fit a model contained within it.



See [here](./policies) for more details.

