---
sidebar_position: 3
title: Basic Concepts
---

<!-- import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem'; -->
import ImgBacktest from '../static/img/schema_backtesting.png';
import ImgOptimization from '../static/img/schema_optimization.png';
import ImgGeneral from '../static/img/env4.png';
import ImgLoop from '../static/img/env3.png';


`dojo's` simulation loop combines principles from [reinforcement learning](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology) and [agent-based modelling](https://en.wikipedia.org/wiki/Agent-based_model). This flexible framework enables you to simulate and explore a diverse range of scenarios that reflect DeFi dynamics.   

The schematic below shows the simplified representation of the agent-environment loop. 

<img src={ImgGeneral} alt="aasd" width="40%" />


To make things easier, let’s consider an example of a trader trading on UniswapV3:  

1. **Environment creation**: as a first step in the simulation, an environment is created. In this case UniswapV3.  
2. **Reset environment**: After initialising the environment, it’s reset to get the first observation from the environment. Observations can be things 
like `block number`, `pool tokens`, `pool token prices`, etc.   
3. **Run policies**: These obervations are turned into actions via your policies (strategies). Think of a trader submitting a trade based on the current price in a pool.  
4. **Process actions**: The environment executes these actions on the forked-blockchain. This can be imagined as Uniswap processing the trader's trade. 
The observations change and the agents compute their rewards for taking their actions.

`dojo` can handle this reward for different scenarios: 


## Static policy performance 
If your policy is static (non-parameter based), the agent’s reward function is simply a measure of performance at every step in the simulation loop.  

<img src={ImgGeneral} alt="aasd" width="50%" />

<!-- <img src=require('@site/static/img/simulation.png').default alt="aasd" width="50px" /> -->

## Optimizing a policy
If you want to optimize model parameters to improve your policy performance, dojo’s feedback loop feeds the reward function back into the policy to iteratively enhance the decision-making process and generate better actions taken by your agent. 

<img src={ImgLoop} alt="aasd" width="50%" />

:::info `dojo` is still in early stage
Become an early adopter and help us shape the product in the way you need!
:::

___
## Terminology

###  Environments
In the context of Dojo, **environments are DeFi protocols, e.g. Uniswap V3, Balancer V2, etc.**  
Environments represent the setting in which an agent interacts and possibly learns.  
At every simulation step (i.e., every block on the blockchain),
the environment emits observations which contain information about the environment, and rewards generated by the agents in the simulation.  
The agent’s policy can process those observations and rewards to take an action. Actions are executed inside  the environment. In practice this means dojo makes the appropriate **smart contract call to our local fork of the DeFi protocol**,
and emits new observations, i.e. the updated state of the chain, back to the simulation.


See [here](./environments/Overview) for more details.

### Agents
Agents represent the state of agents interacting with the environment within dojo’s simulation framework.
This means, the environment has reference to all agents, each of which implement their own reward function for **metric tracking**, 
and, optionally, **policy training**.

:::note

Agents are NOT responsible for making decisions on how to act in the environment.

:::

See [here](./agents) for more details.

### Policies
Policies determine the behavior of the agents. **This is where you can get creative by implementing your own policy for interacting with the environment!** For example, a basic policy could be the [moving average trading strategy](https://www.investopedia.com/ask/answers/122314/how-do-i-use-moving-average-ma-create-forex-trading-strategy.asp).  

At every simulation step (i.e., every block), the policy receives an observation from the environment, with which it generates an action to pass back. Optionally, it also receives a reward which can be used to train or fit a model contained within it.


:::note
Agents act on behalf of their policies.
:::


See [here](./policies) for more details.

