---
sidebar_position: 3
title: Basic Concepts
---

<!-- import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem'; -->
import ImgBacktest from '../static/img/schema_backtesting.png';
import ImgOptimization from '../static/img/schema_optimization.png';
import ImgGeneral from '../static/img/env4.png';


`dojo's` simulation loop combines principles from [reinforcement learning](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology) and [agent-based modelling](https://en.wikipedia.org/wiki/Agent-based_model). This flexible framework enables you to simulate and explore a diverse range of scenarios that reflect DeFi dynamics.   

The schematic below shows the simplified representation of the agent-environment loop. 

<img src={ImgGeneral} alt="aasd" width="40%" />


To make things easier, let’s consider an example of a trader trading on UniswapV3.   

As a first step in the simulation, an environment is created. In this case UniswapV3.  
After initialising the environment, it’s reset to get the first observation from the environment. Observations can be things 
like `block-number`, `pool tokens`, `pool token prices`, etc.   
These obervations are turned into actions via your policies(strategies). Think of a trader making a trade.  
The environment executes these actions on the forked-blockchain. The observations change and the agent computes his 
This can be imagined as the trader doing a trade. This causes a change in the environment, and, as a result, the agent receives 
a new observation from the updated environment along with a reward for taking that action.  

`dojo` can handle this reward for different scenarios: 





### 1. Testing static strategy performance 
If your strategy is static (non-parameter based), the agent’s reward function is simply a measure of performance at every step in the simulation loop.  

<img src={ImgGeneral} alt="aasd" width="50%" />

<!-- <img src=require('@site/static/img/simulation.png').default alt="aasd" width="50px" /> -->
### 2. Optimizing, training and testing strategy performance
If you want to optimize model parameters to improve your strategy performance, dojo’s feedback loop feeds the reward function back into the policy to iteratively enhance the decision-making process and generate better actions taken by your agent. 

<img src={ImgOptimization} alt="aasd" width="50%" />

:::info `dojo` is still in early stage
Become an early adopter and help us shape the product in the way you need!
:::

___
## Terminology

###  *Environments*
In the context of Dojo, **environments are DeFi protocols, e.g. Uniswap V3, Balancer V2, etc.**  
Environments represent the setting in which an agent interacts and possibly learns.  
At every simulation step (i.e., every block on the blockchain),
the environment emits observations which contain information about the environment, and rewards generated by the agents in the simulation.  
The agent’s policy can process those observations and rewards to take an action. Actions can be trades or liquidity provision. Actions are
executed inside  the environment. In practice this means dojo makes the appropriate **smart contract call to our local fork of the DeFi protocol**,
and emits new observations, i.e. the updated state of the chain, back to the simulation.


See [here](./environments/Overview) for more details.

### *Agents*
Agents represent the state of agents interacting with the environment within dojo’s simulation framework.
This means, the environment has reference to all agents, each of which implement their own reward function for **metric tracking**, 
and, optionally, policy training.

:::note

Agents are NOT responsible for making decisions on how to act in the environment.

:::

See [here](./agents) for more details.

### *Policies*
Policies determine the behavior of the agents. **This is where you can get creative by implementing your own policy for interacting with the environment!**  
For example, a basic policy could be the [moving average trading strategy](https://www.investopedia.com/ask/answers/122314/how-do-i-use-moving-average-ma-create-forex-trading-strategy.asp).  

At every simulation step (i.e., every block), the policy receives an observation from the environment, with which it generates an action to pass back. Optionally, it also receives a reward which can be used to train or fit a model contained within it.


:::note
Agents act on behalf of their policies.
:::


See [here](./policies) for more details.

