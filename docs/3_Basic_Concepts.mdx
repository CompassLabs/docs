---
sidebar_position: 3
title: Basic Concepts
---

<!-- import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem'; -->
import ImgBacktest from '../static/img/schema_backtesting.png';
import ImgOptimization from '../static/img/schema_optimization.png';


Dojo’s simulation loop combines principles from [reinforcement learning](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#key-concepts-and-terminology) and [agent-based modelling](https://en.wikipedia.org/wiki/Agent-based_model). This flexible framework enables you to simulate and explore a diverse range of scenarios that reflect DeFi dynamics. 
The schematic below shows the simplified representation of the agent-environment loop. 


___
## What can I do with dojo right now??




### 1. Testing static strategy performance 
If your strategy is static (non-parameter based), the agent’s reward function is simply a measure of performance at every step in the simulation loop.  

<img src={ImgBacktest} alt="aasd" width="50%" />

<!-- <img src=require('@site/static/img/simulation.png').default alt="aasd" width="50px" /> -->
### 2. Optimizing, training and testing strategy performance
If you want to optimize model parameters to improve your strategy performance, dojo’s feedback loop feeds the reward function back into the policy to iteratively enhance the decision-making process and generate better actions taken by your agent. 

<img src={ImgOptimization} alt="aasd" width="50%" />

:::info Dojo is still in early stage
Become an early adopter and help us shape the product in the way you need!
:::

___
## A quick nomenclature

###  *Environments*
In the context of `dojo`, environments are DeFi protocols.
At every simulation step (= every block), the envrionment emits an observation, which contains information about the environment, and the rewards generated by all the agents in the simulation. It also receives actions, which it processes to generate the observation of the next step.

See [here](./environments/Overview) for more details.

### *Agents*
Agents represent the state of the various agents interacting with the environment within the `dojo` simulation. As such, the environmnet has reference to all agents, each of which can implement their own reward function for metric tracking and, optionally, policy training.

:::note

Agents are NOT responsible for making decisions on how to act in the environment.

:::

See [here](./agents) for more details.

### *Policies*
This is where you can get creative by implementing your own policy for interacting with the environmnet!  
For example, a basic policy could be the [moving average trading strategy](https://www.investopedia.com/ask/answers/122314/how-do-i-use-moving-average-ma-create-forex-trading-strategy.asp).
At every simulation step, the policy receives an observation from the environment, with which it generates an action to pass back. Optionally, it also receives a reward which can be used to train or fit a model contained within it.

:::note

Each policy has an agent attached to it, on whose behalf it acts.

:::


See [here](./policies) for more details.

